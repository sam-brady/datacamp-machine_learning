{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELLO NETS\n",
    "\n",
    "# Import the Sequential model and Dense layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer and a hidden layer with 10 neurons\n",
    "model.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n",
    "\n",
    "# Add a 1-neuron output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarise your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTING PARAMETERS\n",
    "\n",
    "\n",
    "# Instantiate a new Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with five neurons and three inputs\n",
    "model.add(Dense(5, input_shape=(3, ), activation=\"relu\"))\n",
    "\n",
    "# Add a final Dense layer with one neuron and no activation\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarize your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD AS SHOWN\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Build the input and hidden layer\n",
    "model.add(Dense(3, input_shape=(2,)))\n",
    "\n",
    "# Add the ouput layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFYING A MODEL\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with 50 neurons and an input of 1 neuron\n",
    "model.add(Dense(50, input_shape=(1,), activation='relu'))\n",
    "\n",
    "# Add two Dense layers with 50 neurons and relu activation\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# End your model with a Dense layer and no activation\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "print(\"Training started..., this can take a while:\")\n",
    "\n",
    "# Fit your model on your data for 30 epochs\n",
    "model.fit(time_steps,y_positions, epochs = 30)\n",
    "\n",
    "# Evaluate your model \n",
    "print(\"Final lost value:\",model.evaluate(time_steps, y_positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING THE ORBIT\n",
    "\n",
    "\n",
    "# Predict the twenty minutes orbit\n",
    "twenty_min_orbit = model.predict(np.arange(-10, 11))\n",
    "\n",
    "# Plot the twenty minute orbit \n",
    "plot_orbit(twenty_min_orbit)\n",
    "\n",
    "\n",
    "# Predict the eighty minute orbit\n",
    "eighty_min_orbit = model.predict(np.arange(-40, 41))\n",
    "\n",
    "# Plot the eighty minute orbit \n",
    "plot_orbit(eighty_min_orbit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORING DOLLAR BILLS\n",
    "\n",
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Use pairplot and set the hue to be our class\n",
    "sns.pairplot(banknotes, hue='class') \n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Describe the data\n",
    "print('Dataset stats: \\n', banknotes.describe())\n",
    "\n",
    "# Count the number of observations of each class\n",
    "print('Observations per class: \\n', banknotes['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A BINARY CLASSIFICATION MODEL\n",
    "\n",
    "# Import the sequential model and dense layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a dense layer \n",
    "model.add(Dense(1, input_shape=(4,), activation='sigmoid'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS THIS DOLLAR BILL FAKE?\n",
    "\n",
    "\n",
    "# Train your model for 20 epochs\n",
    "model.fit(X_train, y_train, epochs=20)\n",
    "\n",
    "# Evaluate your model accuracy on the test set\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A MULTICLASS MODEL\n",
    "\n",
    "# Instantiate a sequential model\n",
    "model = Sequential()\n",
    "  \n",
    "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "  \n",
    "# Add a dense layer with as many neurons as competitors\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "# Compile your model using categorical_crossentropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE YOUR DATASET  (ONE HOT ENCODING)\n",
    "\n",
    "\n",
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes \n",
    "\n",
    "# Print the label encoded competitors\n",
    "print('Label encoded competitors: \\n',darts.competitor.head())\n",
    "\n",
    "\n",
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes \n",
    "\n",
    "# Import to_categorical from keras utils module\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Use to_categorical on your labels\n",
    "coordinates = darts.drop(['competitor'], axis=1)\n",
    "competitors = to_categorical(darts.competitor)\n",
    "\n",
    "# Now print the to_categorical() result\n",
    "print('One-hot encoded competitors: \\n',competitors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ON DART THROWERS\n",
    "\n",
    "# Train your model on the training data for 200 epochs\n",
    "model.fit(coord_train,competitors_train,epochs=200)\n",
    "\n",
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFTMAX PREDICTIONS\n",
    "\n",
    "\n",
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "\n",
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "# Extract the indexes of the highest probable predictions\n",
    "preds = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AN IRRIGATION MACHINE\n",
    "\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "model.add(Dense(64, input_shape=(20,), activation='relu'))\n",
    "\n",
    "# Add an output layer of 3 neurons with sigmoid activation\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "# Compile your model with adam and binary crossentropy loss\n",
    "model.compile(optimizer='adam',\n",
    "           loss='binary_crossentropy',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING WITH MULTIPLE LABELS\n",
    "\n",
    "# Train for 100 epochs using a validation split of 0.2\n",
    "model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2)\n",
    "\n",
    "# Predict on sensors_test and round up the predictions\n",
    "preds = model.predict(sensors_test)\n",
    "preds_rounded = np.round(preds)\n",
    "\n",
    "# Print rounded preds\n",
    "print('Rounded Predictions: \\n', preds_rounded)\n",
    "\n",
    "# Evaluate your model's accuracy on the test data\n",
    "accuracy = model.evaluate(sensors_test , parcels_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE HISTORY CALLBACK\n",
    "\n",
    "\n",
    "# Train your model and save it's history\n",
    "history = model.fit(X_train, y_train, epochs = 50,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(history.history['loss'], history.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(history.history['acc'], history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EARLY STOPING YOUR MODEL\n",
    "\n",
    "# Import the early stopping callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define a callback to monitor val_acc\n",
    "monitor_val_acc = EarlyStopping(monitor='val_acc', \n",
    "                       patience=5)\n",
    "\n",
    "# Train your model using the early stopping callback\n",
    "model.fit(X_train, y_train, \n",
    "           epochs=1000, validation_data=(X_test,y_test),\n",
    "           callbacks=[monitor_val_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A COMBINATION OF CALLBACKS\n",
    "\n",
    "\n",
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor = 'val_acc', patience=3)\n",
    "\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "modelCheckpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)\n",
    "\n",
    "# Fit your model for a stupid amount of epochs\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs = 10000000,\n",
    "                    callbacks = [monitor_val_acc, modelCheckpoint],\n",
    "                    validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Your Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING THE DIGITS\n",
    "\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
    "model.add(Dense(16, input_shape = (64,), activation = 'relu'))\n",
    "\n",
    "# Output layer with 10 neurons (one per digit) and softmax\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Test if your model works and can process input data\n",
    "print(model.predict(X_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS THE MDOEL OVERFITTING?\n",
    "\n",
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "history = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Extract from the history object loss and val_loss to plot the learning curve\n",
    "plot_loss(history.history['loss'], history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO WE NEED MORE DATA?\n",
    "\n",
    "\n",
    "\n",
    "for size in train_sizes:\n",
    "  \t# Get a fraction of training data (we only care about the training data)\n",
    "    X_train_frac, X_test_frac, y_train_frac, y_test_frac = train_test_split(\n",
    "      X_train, y_train, train_size = size)\n",
    "    # Set the model weights and fit the model on the training data\n",
    "    model.set_weights(initial_weights)\n",
    "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
    "\n",
    "    # Evaluate and store the train fraction and the complete test set results\n",
    "    train_accs.append(model.evaluate(X_train_frac, y_train_frac)[1])\n",
    "    test_accs.append(model.evaluate(X_test_frac, y_test_frac)[1])\n",
    "\n",
    "# Plot train vs test accuracies\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARING ACTIVATION FUNCTIONS (train)\n",
    "\n",
    "\n",
    "# Set a seed\n",
    "np.random.seed(27)\n",
    "\n",
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model\n",
    "  history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=0)\n",
    "  activation_results[act] = history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARING ACTIVATION FUNCTIONS (graph all)\n",
    "\n",
    "\n",
    "# Create a dataframe from val_loss_per_function\n",
    "val_loss= pd.DataFrame(val_loss_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_loss.plot()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe from val_acc_per_function\n",
    "val_acc = pd.DataFrame(val_acc_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_acc.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING BATCH SIZES\n",
    "\n",
    "\n",
    "# Get a fresh new model with get_model\n",
    "model = get_model()\n",
    "\n",
    "# Train your model for 5 epochs with a batch size of 1\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=1)\n",
    "print(\"\\n The accuracy when using a batch of size 1 is: \",\n",
    "      model.evaluate(X_test, y_test)[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Fit your model for 5 epochs with a batch of size the training set\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=len(X_train))\n",
    "print(\"\\n The accuracy when using the whole training set as a batch was: \",\n",
    "      model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACTHNORMALIZATION WITH A FAMILIAR MODEL\n",
    "\n",
    "# Import batch normalization from keras layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Build your deep network\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH NORMALIZATION EFFECTS (graphs)\n",
    "\n",
    "# Train your standard model, storing its history\n",
    "history1 = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Train the batch normalized model you recently built, store its history\n",
    "history2 = batchnorm_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Call compare_acc_histories passing in both model histories\n",
    "compare_histories_acc(history1, history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING A MODEL FOR TUNING\n",
    "\n",
    "\n",
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate=0.01, activation='relu'):\n",
    "  \n",
    "  \t# Create an Adam optimizer with the given learning rate\n",
    "  \topt = Adam(lr=learning_rate)\n",
    "  \t\n",
    "  \t# Create your binary classification model  \n",
    "  \tmodel = Sequential()\n",
    "  \tmodel.add(Dense(128, input_shape=(30,), activation=activation))\n",
    "  \tmodel.add(Dense(256, activation=activation))\n",
    "  \tmodel.add(Dense(1, activation='sigmoid'))\n",
    "  \t\n",
    "  \t# Compile your model with your optimizer, loss, and metrics\n",
    "  \tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  \treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNING THE MODEL PARAMETERS\n",
    "\n",
    "\n",
    "# Import KerasClassifier from keras wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "# Define the parameters to try out\n",
    "params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], \n",
    "          'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "# Create a randomize search cv object passing in the parameters to try\n",
    "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    "\n",
    "# Running random_search.fit(X,y) would start the search,but it takes too long! \n",
    "show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING WITH CROSS VALIDATION\n",
    "\n",
    "\n",
    "# Import KerasClassifier from keras wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 50, \n",
    "             batch_size = 128, verbose = 0)\n",
    "\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model, X, y, cv = 3)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITS A FLOW OF TENSORS\n",
    "\n",
    "\n",
    "# Import keras backend\n",
    "import keras.backend as K\n",
    "\n",
    "# Input tensor from the 1st layer of the model\n",
    "inp = model.layers[0].input\n",
    "\n",
    "# Output tensor from the 1st layer of the model\n",
    "out = model.layers[0].output\n",
    "\n",
    "# Define a function from inputs to outputs\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "# Print the results of passing X_test through the 1st layer\n",
    "print(inp_to_out([X_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL SEPARATION\n",
    "\n",
    "\n",
    "for i in range(0, 21):\n",
    "  \t# Train model for 1 epoch\n",
    "    h = model.fit(X_train, y_train, batch_size=16, epochs=1,verbose=0)\n",
    "    if i%4==0: \n",
    "      # Get the output of the first layer\n",
    "      layer_output = inp_to_out([X_test])[0]\n",
    "      \n",
    "      # Evaluate model accuracy for this epoch\n",
    "      test_accuracy = model.evaluate(X_test, y_test)[1] \n",
    "      \n",
    "      # Plot 1st vs 2nd neuron output\n",
    "      plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING AN AUTOENCODER\n",
    "\n",
    "# Start with a sequential model\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Add a dense layer with the original image as input\n",
    "autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
    "\n",
    "# Add an output layer with as many nodes as the image\n",
    "autoencoder.add(Dense(784, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile your model\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "# Take a look at your model structure\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE-NOISING LIKE AN AUTOENCODER\n",
    "\n",
    "\n",
    "\n",
    "# Build your encoder\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Encode the images and show the encodings\n",
    "preds = encoder.predict(X_test_noise)\n",
    "show_encodings(preds)\n",
    "\n",
    "# Predict on the noisy images with your autoencoder\n",
    "decoded_imgs = autoencoder.predict(X_test_noise)\n",
    "\n",
    "# Plot noisy vs decoded images\n",
    "compare_plot(X_test_noise, decoded_imgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING A CNN MODEL\n",
    "\n",
    "# Import the Conv2D and Flatten layers and instantiate model\n",
    "from keras.layers import Conv2D,Flatten\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer of 32 filters of size 3x3\n",
    "model.add(Conv2D(filters=32, input_shape=(28, 28, 1), kernel_size=3, activation='relu'))\n",
    "\n",
    "# Add a convolutional layer of 16 filters of size 3x3\n",
    "model.add(Conv2D(filters=16, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Flatten the previous layer output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add as many outputs as classes with softmax activation\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOKING AT CONVOLUTIONS\n",
    "\n",
    "\n",
    "# Obtain a reference to the outputs of the first layer\n",
    "layer_output = model.layers[0].output\n",
    "\n",
    "# Build a model using the model's input and the first layer output\n",
    "first_layer_model = Model(inputs = model.input, outputs = layer_output)\n",
    "\n",
    "# Use this model to predict on X_test\n",
    "activations = first_layer_model.predict(X_test)\n",
    "\n",
    "# Plot the activations of first digit of X_test for the 15th filter\n",
    "axs[0].matshow(activations[0,:,:,14], cmap = 'viridis')\n",
    "\n",
    "# Do the same but for the 18th filter now\n",
    "axs[1].matshow(activations[0,:,:,18], cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING YOUR INPUT IMAGE\n",
    "\n",
    "\n",
    "# Import image and preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load the image with the right target size for your model\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Turn it into an array\n",
    "img_array = image.image.img_to_array(img)\n",
    "\n",
    "# Expand the dimensions of the image\n",
    "img_expanded = np.expand_dims(img_array, axis = 0)\n",
    "\n",
    "# Pre-process the img in the same way original images were\n",
    "img_ready = preprocess_input(img_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING A REAL WORLD MODEL  (prints top 3)\n",
    "\n",
    "\n",
    "# Instantiate a ResNet50 model with imagenet weights\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Predict with ResNet50 on your already processed img\n",
    "preds = model.predict(img_ready)\n",
    "\n",
    "# Decode predictions\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREDICTION WITH LSTMs\n",
    "\n",
    "\n",
    "# Split text into an array of words\n",
    "words = text.split()\n",
    "\n",
    "# Make lines of 4 words each, moving one word at a time\n",
    "lines = []\n",
    "for i in range(4, len(words)):\n",
    "  lines.append(' '.join(words[i-4:i]))\n",
    "  \n",
    "# Instantiate a Tokenizer, then fit it on the lines\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "\n",
    "# Turn lines into a sequence of numbers\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "print(\"Lines: \\n {} \\n Sequences: \\n {}\".format(lines[:5],sequences[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD YOUR LSTM MODEL\n",
    "\n",
    "\n",
    "\n",
    "# Import the Embedding, LSTM and Dense layer\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with the right parameters\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=8, input_length=3))\n",
    "\n",
    "# Add a 32 unit LSTM layer\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODE YOUR PREDICTIONS\n",
    "\n",
    "\n",
    "def predict_text(test_text):\n",
    "  if len(test_text.split())!=3:\n",
    "    print('Text input should be 3 words!')\n",
    "    return False\n",
    "  \n",
    "  # Turn the test_text into a sequence of numbers\n",
    "  test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "  test_seq = np.array(test_seq)\n",
    "  \n",
    "  # Get the model's next word prediction by passing in test_seq\n",
    "  pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
    "  \n",
    "  # Return the word associated to the predicted index\n",
    "  return tokenizer.index_word[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
